name: Backend CI / CD

on:
  push:
    branches: [ main ]
  workflow_dispatch: {}

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  # Optional: target namespace to deploy backend. If not provided and no ALB, default to 'staging'.
  BACKEND_NAMESPACE: ${{ secrets.BACKEND_NAMESPACE }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        working-directory: backend
        run: |
          npm ci

      - name: Run tests
        working-directory: backend
        run: |
          npm run test || true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Get AWS Account ID
        id: acct
        run: echo "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)" >> $GITHUB_OUTPUT

      - name: Ensure ECR repo exists
        run: |
          ACCOUNT_ID=${{ steps.acct.outputs.ACCOUNT_ID }}
          aws ecr describe-repositories --repository-names ecommerce-ecr-backend || aws ecr create-repository --repository-name ecommerce-ecr-backend

      - name: Build Docker image
        run: |
          ACCOUNT_ID=${{ steps.acct.outputs.ACCOUNT_ID }}
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-backend
          # Tag only with the commit SHA to avoid immutable :latest tag conflicts
          docker build -t $ECR_URI:${{ github.sha }} backend/

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ steps.acct.outputs.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

      - name: Push image to ECR
        run: |
          set -euo pipefail
          ACCOUNT_ID=${{ steps.acct.outputs.ACCOUNT_ID }}
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-backend
          IMAGE_TAG=${{ github.sha }}
          # If the SHA tag already exists in an IMMUTABLE repo, skip pushing and continue.
          if aws ecr describe-images --repository-name ecommerce-ecr-backend --image-ids imageTag=${IMAGE_TAG} >/dev/null 2>&1; then
            echo "Image tag ${IMAGE_TAG} already exists in ECR; skipping push."
          else
            echo "Pushing image ${ECR_URI}:${IMAGE_TAG}"
            if ! docker push $ECR_URI:${IMAGE_TAG}; then
              echo "Warning: push of ${IMAGE_TAG} failed (possibly due to immutability). Proceeding since deploy uses the same tag."
            fi
          fi

          # Also try to push/update the ':latest' tag for environments/manifests that reference it.
          # If the repository enforces immutable tags, this may fail; ignore errors in that case.
          echo "Tagging and pushing ':latest' for convenience (ignore failures on immutable repos)"
          docker tag $ECR_URI:${IMAGE_TAG} $ECR_URI:latest || true
          docker push $ECR_URI:latest || echo "Skipping ':latest' push (repo may be immutable)"

  deploy-to-k8s:
    runs-on: ubuntu-latest
    needs: build-and-push
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          # Fix common apiVersion typo (kxs -> k8s) and normalize exec plugin apiVersion to v1
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          # Ensure exec auth plugins won't prompt for input in CI by setting interactiveMode: Never
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Verify kubeconfig
        run: |
          echo "Using KUBECONFIG=$KUBECONFIG"
          kubectl version --client

      - name: Determine target namespace
        run: |
          if [ -n "${{ env.BACKEND_NAMESPACE }}" ]; then
            echo "TARGET_NAMESPACE=${{ env.BACKEND_NAMESPACE }}" >> $GITHUB_ENV
            echo "Using namespace from secret BACKEND_NAMESPACE: ${{ env.BACKEND_NAMESPACE }}"
          elif [ -n "${{ secrets.FRONTEND_NAMESPACE }}" ]; then
            echo "TARGET_NAMESPACE=${{ secrets.FRONTEND_NAMESPACE }}" >> $GITHUB_ENV
            echo "Using FRONTEND_NAMESPACE for backend too: ${{ secrets.FRONTEND_NAMESPACE }}"
          elif [ -n "${{ secrets.FRONTEND_ALB_HOSTNAME }}" ]; then
            echo "TARGET_NAMESPACE=default" >> $GITHUB_ENV
            echo "Defaulting backend namespace to 'default' because FRONTEND_ALB_HOSTNAME is set"
          else
            echo "TARGET_NAMESPACE=staging" >> $GITHUB_ENV
            echo "No namespace provided; defaulting to 'staging'"
          fi

      - name: Ensure target namespace exists (skip for default)
        run: |
          if [ "$TARGET_NAMESPACE" != "default" ]; then
            kubectl get namespace "$TARGET_NAMESPACE" || kubectl create namespace "$TARGET_NAMESPACE"
          else
            echo "Namespace 'default' assumed to exist; skipping creation"
          fi

      - name: Apply backend manifests (Service, Deployment, SA, Secret)
        run: |
          kubectl apply -f k8s/backend-deployment.yaml -n "$TARGET_NAMESPACE" || true
          kubectl apply -f k8s/service-account.yaml -n "$TARGET_NAMESPACE" || true
          kubectl apply -f k8s/secret-jwt.yaml -n "$TARGET_NAMESPACE" || true

      - name: Replace backend deployment image with new tag
        run: |
          set -euo pipefail
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-backend:${{ github.sha }}

          # Temporariamente reduzir para 1 réplica para evitar bloqueios de rollout por capacidade/IPs
          CURRENT_REPLICAS=$(kubectl get deployment ecommerce-backend -n "$TARGET_NAMESPACE" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "2")
          echo "Current replicas: ${CURRENT_REPLICAS}"
          if [ -z "${CURRENT_REPLICAS}" ]; then CURRENT_REPLICAS=2; fi
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Scaling down ecommerce-backend to 1 replica for rollout"
            kubectl scale deployment ecommerce-backend -n "$TARGET_NAMESPACE" --replicas=1 || true
            kubectl rollout status deployment/ecommerce-backend -n "$TARGET_NAMESPACE" --timeout=120s || true
          fi

          echo "Updating image to ${ECR_URI}"
          kubectl set image deployment/ecommerce-backend -n "$TARGET_NAMESPACE" backend=$ECR_URI --record
          kubectl rollout restart deployment/ecommerce-backend -n "$TARGET_NAMESPACE" || true
          kubectl rollout status deployment/ecommerce-backend -n "$TARGET_NAMESPACE" --timeout=180s || true

          # Restaurar número original de réplicas
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Restoring ecommerce-backend replicas to ${CURRENT_REPLICAS}"
            kubectl scale deployment ecommerce-backend -n "$TARGET_NAMESPACE" --replicas=${CURRENT_REPLICAS} || true
            kubectl rollout status deployment/ecommerce-backend -n "$TARGET_NAMESPACE" --timeout=180s || true
          fi
