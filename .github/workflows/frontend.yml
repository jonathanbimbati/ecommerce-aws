name: Frontend CI / CD

on:
  push:
    branches: [ main ]
  workflow_dispatch: {}

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  # Compose an image tag unique per workflow run to avoid immutable-tag overwrite
  IMAGE_TAG: ${{ github.sha }}-${{ github.run_id }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install frontend dependencies
        working-directory: frontend
        run: |
          npm ci

      - name: Build frontend
        working-directory: frontend
        run: |
          npm run build --if-present || true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure ECR repo exists
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          aws ecr describe-repositories --repository-names ecommerce-ecr-frontend || aws ecr create-repository --repository-name ecommerce-ecr-frontend

      - name: Build Docker image
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          # Tag with a unique value combining commit SHA and run id to avoid immutable tag conflicts
          docker build -t $ECR_URI:${IMAGE_TAG} frontend/

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $(aws sts get-caller-identity --query Account --output text).dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

      - name: Push image to ECR
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          # Only push the SHA-tagged image. Avoid pushing or overwriting :latest when the repo is immutable.
          docker push $ECR_URI:${IMAGE_TAG}

  deploy-to-k8s:
    runs-on: ubuntu-latest
    needs: build-and-push
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          # Fix common apiVersion typo (kxs -> k8s) and force supported client auth apiVersion 'v1'
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          # Ensure exec auth plugins won't prompt for input in CI by setting interactiveMode: Never
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Verify kubeconfig
        run: |
          echo "Using KUBECONFIG=$KUBECONFIG"
          kubectl version --client

      - name: Replace deployment image with new sha tag
        run: |
          set -euo pipefail
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend:${IMAGE_TAG}

          # Temporarily scale to 1 replica to avoid scheduling/IP exhaustion issues during rollout
          CURRENT_REPLICAS=$(kubectl get deployment ecommerce-frontend -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "2")
          echo "Current replicas: ${CURRENT_REPLICAS}"
          if [ -z "${CURRENT_REPLICAS}" ]; then CURRENT_REPLICAS=2; fi
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Scaling down ecommerce-frontend to 1 replica for rollout"
            kubectl scale deployment ecommerce-frontend --replicas=1 || true
            kubectl rollout status deployment/ecommerce-frontend --timeout=120s || true
          fi

          echo "Updating image to ${ECR_URI}"
          kubectl set image deployment/ecommerce-frontend frontend=$ECR_URI --record
          kubectl rollout restart deployment/ecommerce-frontend || true
          kubectl rollout status deployment/ecommerce-frontend --timeout=180s || true

          # Restore original replica count if it was > 1
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Restoring ecommerce-frontend replicas to ${CURRENT_REPLICAS}"
            kubectl scale deployment ecommerce-frontend --replicas=${CURRENT_REPLICAS} || true
            kubectl rollout status deployment/ecommerce-frontend --timeout=180s || true
          fi

  prepare-e2e:
    runs-on: ubuntu-latest
    needs: deploy-to-k8s
    outputs:
      enable_e2e: ${{ steps.flag.outputs.enable_e2e }}
    steps:
      - name: Compute E2E enable flag from secret
        id: flag
        run: |
          if [ "${{ secrets.ENABLE_E2E }}" = "true" ]; then
            echo "enable_e2e=true" >> $GITHUB_OUTPUT
          else
            echo "enable_e2e=false" >> $GITHUB_OUTPUT
          fi

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [deploy-to-k8s, prepare-e2e]
    if: ${{ needs.prepare-e2e.outputs.enable_e2e == 'true' }}
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Resolve frontend URL (prefer fixed ALB; fallback to Service LB)
        run: |
          set -e
          # 1) Prefer an explicit ALB hostname provided via repository secret FRONTEND_ALB_HOSTNAME
          if [ -n "${{ secrets.FRONTEND_ALB_HOSTNAME }}" ]; then
            echo "Using FRONTEND_ALB_HOSTNAME secret"
            echo "FRONTEND_URL=http://${{ secrets.FRONTEND_ALB_HOSTNAME }}" >> $GITHUB_ENV
            exit 0
          fi

          # 2) Fallback: try to resolve a LoadBalancer Service in default or staging namespaces
          echo "No FRONTEND_ALB_HOSTNAME secret set; attempting to resolve Service LoadBalancer hostname"
          for i in {1..20}; do
            HOST=$(kubectl get svc ecommerce-frontend -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -z "$HOST" ]; then
              HOST=$(kubectl get svc ecommerce-frontend -n staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            fi
            if [ -n "$HOST" ]; then
              echo "Found LB host: $HOST"; echo "FRONTEND_URL=http://$HOST" >> $GITHUB_ENV; exit 0
            fi
            echo "Waiting for frontend Service LB host... ($i)"; sleep 10
          done
          echo "LoadBalancer hostname not available and no ALB secret provided"; exit 1

      - name: Install system deps and Chromium for Puppeteer
        run: |
          # Install system dependencies required by Chromium/Puppeteer
          sudo apt-get update
          # Some Ubuntu images (24.04) provide libasound2t64 instead of libasound2.
          # Try installing with libasound2t64 first; if that fails, retry with libasound2.
          COMMON_PKGS="ca-certificates fonts-liberation libatk1.0-0 libatk-bridge2.0-0 libcups2 libxrandr2 libxss1 libgbm1 libgtk-3-0 libnss3 libx11-xcb1 chromium"
          for ALSA_PKG in libasound2t64 libasound2; do
            echo "Attempting apt-get install with ALSA package: $ALSA_PKG"
            if sudo apt-get install -y --no-install-recommends $COMMON_PKGS $ALSA_PKG; then
              echo "Successfully installed dependencies with $ALSA_PKG"
              break
            else
              echo "Failed to install $ALSA_PKG. Trying next option..."
            fi
          done

      - name: Install puppeteer deps
        env:
          # Tell puppeteer to skip downloading its bundled Chromium and use system Chromium
          PUPPETEER_SKIP_CHROMIUM_DOWNLOAD: 'true'
          PUPPETEER_EXECUTABLE_PATH: '/snap/bin/chromium'
        run: |
          cd backend/tests/e2e-ui
          npm ci

      - name: Run E2E Puppeteer tests
        run: |
          cd backend/tests/e2e-ui
          npm run test

