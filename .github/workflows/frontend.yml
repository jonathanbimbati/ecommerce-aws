name: Frontend CI / CD

on:
  push:
    branches: [ main ]
  workflow_dispatch: {}

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  # Compose an image tag unique per workflow run to avoid immutable-tag overwrite
  IMAGE_TAG: ${{ github.sha }}-${{ github.run_id }}
  # Gate to enable/disable E2E; defaults to empty/false if secret not set
  ENABLE_E2E: ${{ secrets.ENABLE_E2E }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install frontend dependencies
        working-directory: frontend
        run: |
          npm ci

      - name: Build frontend
        working-directory: frontend
        run: |
          npm run build --if-present || true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure ECR repo exists
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          aws ecr describe-repositories --repository-names ecommerce-ecr-frontend || aws ecr create-repository --repository-name ecommerce-ecr-frontend

      - name: Build Docker image
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          # Tag with a unique value combining commit SHA and run id to avoid immutable tag conflicts
          docker build -t $ECR_URI:${IMAGE_TAG} frontend/

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $(aws sts get-caller-identity --query Account --output text).dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

      - name: Push image to ECR
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          # Only push the SHA-tagged image. Avoid pushing or overwriting :latest when the repo is immutable.
          docker push $ECR_URI:${IMAGE_TAG}

  deploy-to-k8s:
    runs-on: ubuntu-latest
    needs: build-and-push
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          # Fix common apiVersion typo (kxs -> k8s) and force supported client auth apiVersion 'v1'
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          # Ensure exec auth plugins won't prompt for input in CI by setting interactiveMode: Never
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Verify kubeconfig
        run: |
          echo "Using KUBECONFIG=$KUBECONFIG"
          kubectl version --client

      - name: Determine target namespace
        run: |
          if [ -n "${{ secrets.FRONTEND_NAMESPACE }}" ]; then
            echo "TARGET_NAMESPACE=${{ secrets.FRONTEND_NAMESPACE }}" >> $GITHUB_ENV
            echo "Using namespace from secret FRONTEND_NAMESPACE: ${{ secrets.FRONTEND_NAMESPACE }}"
          else
            # Default to 'staging' to keep frontend and backend aligned unless explicitly overridden
            echo "TARGET_NAMESPACE=staging" >> $GITHUB_ENV
            echo "Defaulting namespace to 'staging' (set FRONTEND_NAMESPACE to override)"
          fi

      - name: Ensure target namespace exists (skip for default)
        run: |
          if [ "$TARGET_NAMESPACE" != "default" ]; then
            kubectl get namespace "$TARGET_NAMESPACE" || kubectl create namespace "$TARGET_NAMESPACE"
          else
            echo "Namespace 'default' assumed to exist; skipping creation"
          fi

      - name: Apply manifests to target namespace (frontend + backend)
        run: |
          kubectl apply -f k8s/frontend-deployment.yaml -n "$TARGET_NAMESPACE" || true
          kubectl apply -f k8s/backend-deployment.yaml -n "$TARGET_NAMESPACE" || true
          kubectl apply -f k8s/service-account.yaml -n "$TARGET_NAMESPACE" || true
          kubectl apply -f k8s/secret-jwt.yaml -n "$TARGET_NAMESPACE" || true
          kubectl apply -f k8s/frontend-hpa.yaml -n "$TARGET_NAMESPACE" || true

      - name: Wait backend rollout in target namespace
        run: |
          set -e
          echo "Waiting for ecommerce-backend rollout in namespace $TARGET_NAMESPACE"
          kubectl rollout status deployment/ecommerce-backend -n "$TARGET_NAMESPACE" --timeout=240s || (
            echo "Backend did not become ready in time; dumping describe and pods" &&
            kubectl describe deployment/ecommerce-backend -n "$TARGET_NAMESPACE" || true &&
            kubectl get pods -n "$TARGET_NAMESPACE" -l app=ecommerce-backend -o wide || true &&
            exit 1
          )

      - name: Replace deployment image with new sha tag (target namespace)
        run: |
          set -euo pipefail
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend:${IMAGE_TAG}

          # Temporarily scale to 1 replica to avoid scheduling/IP exhaustion issues during rollout
          CURRENT_REPLICAS=$(kubectl get deployment ecommerce-frontend -n "$TARGET_NAMESPACE" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "2")
          echo "Current replicas: ${CURRENT_REPLICAS}"
          if [ -z "${CURRENT_REPLICAS}" ]; then CURRENT_REPLICAS=2; fi
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Scaling down ecommerce-frontend to 1 replica for rollout"
            kubectl scale deployment ecommerce-frontend -n "$TARGET_NAMESPACE" --replicas=1 || true
            kubectl rollout status deployment/ecommerce-frontend -n "$TARGET_NAMESPACE" --timeout=120s || true
          fi

          echo "Updating image to ${ECR_URI}"
          kubectl set image deployment/ecommerce-frontend -n "$TARGET_NAMESPACE" frontend=$ECR_URI --record
          kubectl rollout restart deployment/ecommerce-frontend -n "$TARGET_NAMESPACE" || true
          kubectl rollout status deployment/ecommerce-frontend -n "$TARGET_NAMESPACE" --timeout=180s || true

          # Restore original replica count if it was > 1
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Restoring ecommerce-frontend replicas to ${CURRENT_REPLICAS}"
            kubectl scale deployment ecommerce-frontend -n "$TARGET_NAMESPACE" --replicas=${CURRENT_REPLICAS} || true
            kubectl rollout status deployment/ecommerce-frontend -n "$TARGET_NAMESPACE" --timeout=180s || true
          fi

  e2e-tests:
    runs-on: ubuntu-latest
    needs: deploy-to-k8s
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Determine target namespace for E2E
        run: |
          if [ -n "${{ secrets.FRONTEND_NAMESPACE }}" ]; then
            echo "TARGET_NAMESPACE=${{ secrets.FRONTEND_NAMESPACE }}" >> $GITHUB_ENV
            echo "E2E using namespace from secret FRONTEND_NAMESPACE: ${{ secrets.FRONTEND_NAMESPACE }}"
          elif [ -n "${{ secrets.FRONTEND_ALB_HOSTNAME }}" ]; then
            echo "TARGET_NAMESPACE=default" >> $GITHUB_ENV
            echo "E2E defaulting namespace to 'default' because FRONTEND_ALB_HOSTNAME is set"
          else
            echo "TARGET_NAMESPACE=staging" >> $GITHUB_ENV
            echo "E2E defaulting namespace to 'staging' (no fixed ALB)"
          fi

      - name: Resolve frontend URL (prefer fixed ALB; fallback to Service LB in target namespace)
        run: |
          set -e
          # 1) Prefer an explicit ALB hostname provided via repository secret FRONTEND_ALB_HOSTNAME
          if [ -n "${{ secrets.FRONTEND_ALB_HOSTNAME }}" ]; then
            echo "Using FRONTEND_ALB_HOSTNAME secret"
            echo "FRONTEND_URL=http://${{ secrets.FRONTEND_ALB_HOSTNAME }}" >> $GITHUB_ENV
            exit 0
          fi

          # 2) Fallback: try to resolve a LoadBalancer Service in the target namespace
          echo "No FRONTEND_ALB_HOSTNAME secret set; attempting to resolve Service LoadBalancer hostname in namespace $TARGET_NAMESPACE"
          for i in {1..20}; do
            if [ -z "$HOST" ]; then
              HOST=$(kubectl get svc ecommerce-frontend -n "$TARGET_NAMESPACE" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            fi
            if [ -n "$HOST" ]; then
              echo "Found LB host: $HOST"; echo "FRONTEND_URL=http://$HOST" >> $GITHUB_ENV; exit 0
            fi
            echo "Waiting for frontend Service LB host... ($i)"; sleep 10
          done
          echo "LoadBalancer hostname not available and no ALB secret provided"; exit 1
      - name: Probe backend API via frontend LB (/api) before E2E
        run: |
          set -e
          mkdir -p backend/tests/e2e-ui/artifacts || true
          if [ -z "$FRONTEND_URL" ]; then
            echo "FRONTEND_URL ausente; não é possível sondar backend" | tee backend/tests/e2e-ui/artifacts/backend-probe-error.txt
            exit 1
          fi
          PROBE_URL="$FRONTEND_URL/api/products"
          echo "$PROBE_URL" > backend/tests/e2e-ui/artifacts/backend-probe-url.txt
          echo "Probing: $PROBE_URL"
          set -o pipefail
          curl -sv --connect-timeout 8 --max-time 20 "$PROBE_URL" \
            -o backend/tests/e2e-ui/artifacts/backend-probe-body.json \
            -D backend/tests/e2e-ui/artifacts/backend-probe-headers.txt \
            2> backend/tests/e2e-ui/artifacts/backend-probe-curl.txt || PROBE_RC=$?

          STATUS=$(awk 'toupper($1) ~ /^HTTP\// {code=$2} END{print code+0}' backend/tests/e2e-ui/artifacts/backend-probe-headers.txt 2>/dev/null || echo 0)
          echo "HTTP STATUS: $STATUS" | tee -a backend/tests/e2e-ui/artifacts/backend-probe-headers.txt
          if [ -n "$PROBE_RC" ]; then
            echo "Curl retornou código $PROBE_RC" | tee backend/tests/e2e-ui/artifacts/backend-probe-error.txt
            exit 1
          fi
          if [ "$STATUS" -lt 200 ] || [ "$STATUS" -ge 400 ]; then
            echo "Backend probe falhou com status $STATUS" | tee backend/tests/e2e-ui/artifacts/backend-probe-error.txt
            exit 1
          fi
          echo "Backend OK (status $STATUS)"
      - name: Install system deps and Chromium for Puppeteer
        run: |
          # Install system dependencies required by Chromium/Puppeteer
          sudo apt-get update
          # Some Ubuntu images (24.04) provide libasound2t64 instead of libasound2.
          # Try installing with libasound2t64 first; if that fails, retry with libasound2.
          COMMON_PKGS="ca-certificates fonts-liberation libatk1.0-0 libatk-bridge2.0-0 libcups2 libxrandr2 libxss1 libgbm1 libgtk-3-0 libnss3 libx11-xcb1 chromium"
          for ALSA_PKG in libasound2t64 libasound2; do
            echo "Attempting apt-get install with ALSA package: $ALSA_PKG"
            if sudo apt-get install -y --no-install-recommends $COMMON_PKGS $ALSA_PKG; then
              echo "Successfully installed dependencies with $ALSA_PKG"
              break
            else
              echo "Failed to install $ALSA_PKG. Trying next option..."
            fi
          done
          # Persist Puppeteer env so subsequent steps inherit the executable path
          echo "PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true" >> $GITHUB_ENV
          echo "PUPPETEER_EXECUTABLE_PATH=/snap/bin/chromium" >> $GITHUB_ENV

      - name: Install puppeteer deps
        run: |
          cd backend/tests/e2e-ui
          npm ci

      - name: Run E2E Puppeteer tests
        env:
          # Allow overriding the removal wait in CI if needed
          E2E_REMOVE_TIMEOUT_MS: '90000'
        run: |
          cd backend/tests/e2e-ui
          npm run test

      - name: Debug list E2E artifact directory (for troubleshooting)
        if: ${{ always() }}
        run: |
          echo "Workspace: $(pwd)"
          echo "Listing backend/tests/e2e-ui (top level)" || true
          ls -la backend/tests/e2e-ui || true
          echo "Listing backend/tests/e2e-ui/artifacts (if exists)" || true
          ls -la backend/tests/e2e-ui/artifacts || true
          echo "Recursive find (show up to 3 levels)" || true
          find backend/tests/e2e-ui -maxdepth 3 -type f -print || true

      - name: Upload E2E artifacts (if any)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: e2e-artifacts
          path: backend/tests/e2e-ui/artifacts/**

