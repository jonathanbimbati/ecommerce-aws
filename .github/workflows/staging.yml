name: Staging pipeline (build -> deploy -> test -> promote)

on:
  push:
    branches: [ main ]
  workflow_dispatch: {}
env:
  AWS_REGION: ${{ secrets.AWS_REGION }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    outputs:
      image_sha: ${{ steps.set-vars.outputs.IMAGE_SHA }}
      staging_tag: ${{ steps.set-vars.outputs.STAGING_TAG }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install frontend deps and build
        working-directory: frontend
        run: |
          npm ci
          npm run build --if-present || true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure ECR repo exists
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          aws ecr describe-repositories --repository-names ecommerce-ecr-frontend || aws ecr create-repository --repository-name ecommerce-ecr-frontend

      - name: Build Docker image and push tags
        id: set-vars
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          IMAGE_SHA=${{ github.sha }}
          STAGING_TAG=staging-${IMAGE_SHA}
          docker build -t $ECR_URI:${IMAGE_SHA} frontend/
          docker tag $ECR_URI:${IMAGE_SHA} $ECR_URI:${STAGING_TAG}
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

          # Push logic that is safe for IMMUTABLE tag repositories:
          # - If IMAGE_SHA already exists, ensure STAGING_TAG exists but do not attempt to overwrite it.
          # - If IMAGE_SHA does not exist, push IMAGE_SHA and create STAGING_TAG (unless it already exists).
          if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${IMAGE_SHA} >/dev/null 2>&1; then
            echo "Image tag ${IMAGE_SHA} already exists in ECR; ensuring staging tag ${STAGING_TAG} exists without overwriting."
            if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${STAGING_TAG} >/dev/null 2>&1; then
              echo "Staging tag ${STAGING_TAG} already exists. Skipping push."
            else
              echo "Pushing only staging tag ${STAGING_TAG}"
              docker push $ECR_URI:${STAGING_TAG}
            fi
          else
            echo "Pushing image ${ECR_URI}:${IMAGE_SHA} and tag ${STAGING_TAG} (if staging tag not already present)"
            docker push $ECR_URI:${IMAGE_SHA}
            if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${STAGING_TAG} >/dev/null 2>&1; then
              echo "Staging tag ${STAGING_TAG} already exists after pushing IMAGE_SHA. Skipping pushing staging tag."
            else
              docker push $ECR_URI:${STAGING_TAG}
            fi
          fi

          # Expose both IMAGE_SHA and STAGING_TAG so downstream steps can choose the exact tag to use
          echo "IMAGE_SHA=${IMAGE_SHA}" >> $GITHUB_OUTPUT
          echo "STAGING_TAG=${STAGING_TAG}" >> $GITHUB_OUTPUT
          echo "ECR_URI=${ECR_URI}" >> $GITHUB_OUTPUT

  deploy-and-test-staging:
    runs-on: ubuntu-latest
    needs: build-and-push
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure AWS credentials for kubeconfig exec-plugin
        if: ${{ env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Refresh kubeconfig via AWS (ensure exec-plugin uses job credentials)
        if: ${{ env.AWS_ACCESS_KEY_ID != '' && env.EKS_CLUSTER_NAME != '' }}
        run: |
          echo "Refreshing kubeconfig using AWS CLI so exec-plugin picks up credentials"
          aws sts get-caller-identity || true
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }} --kubeconfig $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}

      - name: Create staging namespace
        run: |
          kubectl get namespace staging || kubectl create namespace staging

      - name: Apply frontend k8s manifests to staging
        run: |
          kubectl apply -f k8s/frontend-deployment.yaml -n staging || true
          # Apply backend service alias so legacy hostname 'backend' resolves to ecommerce-backend
          kubectl apply -f k8s/backend-service-alias.yaml -n staging || true

      - name: Replace deployment image with staging sha tag
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          # Prefer the explicitly pushed unique STAGING_TAG when available to avoid 'latest' or immutable-tag issues
          STAGING_TAG=${{ needs.build-and-push.outputs.staging_tag }}
          IMAGE_TO_USE=${STAGING_TAG:-${{ needs.build-and-push.outputs.image_sha }}}
          echo "Using image tag: ${IMAGE_TO_USE}"
          kubectl set image deployment/ecommerce-frontend -n staging frontend=$ECR_URI:${IMAGE_TO_USE} --record
          # Force a rollout restart to ensure every pod pulls the image (helps avoid stale cached images on nodes)
          kubectl rollout restart deployment/ecommerce-frontend -n staging || true
          kubectl rollout status deployment/ecommerce-frontend -n staging --timeout=180s

      - name: Wait for LoadBalancer hostname (staging service)
        run: |
          for i in {1..20}; do
            HOST=$(kubectl get svc ecommerce-frontend -n staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$HOST" ]; then
              echo "FRONTEND_HOST=$HOST" >> $GITHUB_ENV
              echo "Found LB host: $HOST"; break
            fi
            echo "Waiting for LB host... ($i)"; sleep 15
          done
          if [ -z "$HOST" ]; then echo "LoadBalancer hostname not available"; exit 1; fi

      - name: Run E2E tests against staging
        env:
          FRONTEND_URL: https://${{ env.FRONTEND_HOST }}
          PUPPETEER_SKIP_CHROMIUM_DOWNLOAD: 'true'
          PUPPETEER_EXECUTABLE_PATH: '/snap/bin/chromium'
        run: |
          # Create artifacts dir for diagnostic outputs
          mkdir -p backend/tests/e2e-ui/artifacts || true

          # Collect TLS diagnostic information from the LB before running E2E tests.
          # Save headers and body (curl -vk will show TLS handshake info) and collect the cert chain via openssl.
          echo "Collecting TLS diagnostic info for: ${{ env.FRONTEND_HOST }}"
          # Save curl verbose output (headers and TLS handshake traces)
          curl -vk "https://${{ env.FRONTEND_HOST }}" -D backend/tests/e2e-ui/artifacts/ssl-headers.txt -o backend/tests/e2e-ui/artifacts/ssl-body.html || true
          # Save openssl s_client cert chain and connection diagnostics
          echo "-----BEGIN OPENSSL S_CLIENT OUTPUT-----" > backend/tests/e2e-ui/artifacts/openssl-s_client.txt || true
          echo "Connecting to ${{ env.FRONTEND_HOST }}:443" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt || true
          openssl s_client -showcerts -servername "${{ env.FRONTEND_HOST }}" -connect "${{ env.FRONTEND_HOST }}:443" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt 2>&1 || true
          echo "-----END OPENSSL S_CLIENT OUTPUT-----" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt || true

          # Run the E2E tests
          cd backend/tests/e2e-ui
          npm ci
          npm run test

      - name: Debug list E2E artifact directory (for troubleshooting)
        if: ${{ always() }}
        run: |
          echo "Workspace: $(pwd)"
          echo "Listing backend/tests/e2e-ui (top level)" || true
          ls -la backend/tests/e2e-ui || true
          echo "Listing backend/tests/e2e-ui/artifacts (if exists)" || true
          ls -la backend/tests/e2e-ui/artifacts || true
          echo "Recursive find (show up to 3 levels)" || true
          find backend/tests/e2e-ui -maxdepth 3 -type f -print || true

      - name: Upload E2E artifacts (if any)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: e2e-artifacts
          path: backend/tests/e2e-ui/artifacts/**

      - name: Promote to production (automatic)
        if: ${{ success() }}
        run: |
          echo "Promotion: updating production deployment to use staging image"
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          IMAGE_SHA=${{ needs.build-and-push.outputs.image_sha }}
          kubectl set image deployment/ecommerce-frontend frontend=$ECR_URI:${IMAGE_SHA} --record
          kubectl rollout status deployment/ecommerce-frontend --timeout=180s
