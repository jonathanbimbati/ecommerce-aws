name: Staging pipeline (build -> deploy -> test -> promote)

on:
  push:
    branches: [ main ]
  workflow_dispatch: {}
env:
  AWS_REGION: ${{ secrets.AWS_REGION }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    outputs:
      image_sha: ${{ steps.set-vars.outputs.IMAGE_SHA }}
      staging_tag: ${{ steps.set-vars.outputs.STAGING_TAG }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install frontend deps and build
        working-directory: frontend
        run: |
          npm ci
          npm run build --if-present || true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure ECR repo exists
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          aws ecr describe-repositories --repository-names ecommerce-ecr-frontend || aws ecr create-repository --repository-name ecommerce-ecr-frontend

      - name: Build Docker image and push tags
        id: set-vars
        run: |
          set -euo pipefail
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          IMAGE_SHA=${{ github.sha }}
          STAGING_TAG=staging-${IMAGE_SHA}
          docker build -t $ECR_URI:${IMAGE_SHA} frontend/
          docker tag $ECR_URI:${IMAGE_SHA} $ECR_URI:${STAGING_TAG}
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com

          # Safer push logic for IMMUTABLE tag repositories:
          # 1) If IMAGE_SHA tag already exists in ECR, skip pushing it and try to ensure STAGING_TAG exists.
          # 2) Otherwise, try to push IMAGE_SHA. If that push fails with an immutable-tag error,
          #    attempt to push only STAGING_TAG. If STAGING_TAG also exists, skip.
          if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${IMAGE_SHA} >/dev/null 2>&1; then
            echo "Image tag ${IMAGE_SHA} already exists in ECR; ensuring staging tag ${STAGING_TAG} exists without overwriting."
            if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${STAGING_TAG} >/dev/null 2>&1; then
              echo "Staging tag ${STAGING_TAG} already exists. Skipping push."
            else
              echo "Pushing only staging tag ${STAGING_TAG}"
              docker push $ECR_URI:${STAGING_TAG} || echo "Warning: push of ${STAGING_TAG} failed; it may already exist or another error occurred."
            fi
          else
            echo "Attempting to push image ${ECR_URI}:${IMAGE_SHA}"
            if docker push $ECR_URI:${IMAGE_SHA}; then
              echo "Pushed ${IMAGE_SHA} successfully"
              if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${STAGING_TAG} >/dev/null 2>&1; then
                echo "Staging tag ${STAGING_TAG} already exists after pushing IMAGE_SHA. Skipping pushing staging tag."
              else
                echo "Pushing staging tag ${STAGING_TAG}"
                docker push $ECR_URI:${STAGING_TAG} || echo "Warning: failed to push ${STAGING_TAG}; continuing if tag already exists"
              fi
            else
              echo "Push of ${IMAGE_SHA} failed; attempting to push only ${STAGING_TAG} to ensure a usable tag is present"
              if aws ecr describe-images --repository-name ecommerce-ecr-frontend --image-ids imageTag=${STAGING_TAG} >/dev/null 2>&1; then
                echo "Staging tag ${STAGING_TAG} already exists. Skipping push."
              else
                docker push $ECR_URI:${STAGING_TAG} || echo "Warning: push of ${STAGING_TAG} failed; it may already exist or ECR rejected the tag."
              fi
            fi
          fi

          # Expose both IMAGE_SHA and STAGING_TAG so downstream steps can choose the exact tag to use
          echo "IMAGE_SHA=${IMAGE_SHA}" >> $GITHUB_OUTPUT
          echo "STAGING_TAG=${STAGING_TAG}" >> $GITHUB_OUTPUT
          echo "ECR_URI=${ECR_URI}" >> $GITHUB_OUTPUT

  deploy-and-test-staging:
    runs-on: ubuntu-latest
    needs: build-and-push
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure AWS credentials for kubeconfig exec-plugin
        if: ${{ env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Refresh kubeconfig via AWS (ensure exec-plugin uses job credentials)
        if: ${{ env.AWS_ACCESS_KEY_ID != '' && env.EKS_CLUSTER_NAME != '' }}
        run: |
          echo "Refreshing kubeconfig using AWS CLI so exec-plugin picks up credentials"
          aws sts get-caller-identity || true
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }} --kubeconfig $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}

      - name: Create staging namespace
        run: |
          kubectl get namespace staging || kubectl create namespace staging

      - name: Apply frontend k8s manifests to staging
        run: |
          kubectl apply -f k8s/frontend-deployment.yaml -n staging || true
          # Apply backend ExternalName service so 'backend' DNS resolves to API Gateway host
          kubectl apply -f k8s/backend-deployment.yaml -n staging || true

      - name: Apply HPA (frontend) to staging
        run: |
          kubectl apply -f k8s/frontend-hpa.yaml -n staging || true

      - name: Replace deployment image with staging sha tag
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          # Prefer the explicitly pushed unique STAGING_TAG when available to avoid 'latest' or immutable-tag issues
          STAGING_TAG=${{ needs.build-and-push.outputs.staging_tag }}
          IMAGE_TO_USE=${STAGING_TAG:-${{ needs.build-and-push.outputs.image_sha }}}
          echo "Using image tag: ${IMAGE_TO_USE}"

          # Temporarily reduce replicas to 1 to make rollout tolerant to small clusters
          CURRENT_REPLICAS=$(kubectl get deployment ecommerce-frontend -n staging -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "2")
          echo "Current replicas: ${CURRENT_REPLICAS}"
          if [ -z "${CURRENT_REPLICAS}" ]; then CURRENT_REPLICAS=2; fi
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Scaling down ecommerce-frontend to 1 replica to allow rollout on constrained cluster"
            kubectl scale deployment ecommerce-frontend -n staging --replicas=1 || true
            # Wait a short time for scaling to settle
            kubectl rollout status deployment/ecommerce-frontend -n staging --timeout=120s || true
          else
            echo "Deployment already at 1 replica"
          fi

          # Update image and trigger rollout
          kubectl set image deployment/ecommerce-frontend -n staging frontend=$ECR_URI:${IMAGE_TO_USE} --record
          kubectl rollout restart deployment/ecommerce-frontend -n staging || true
          kubectl rollout status deployment/ecommerce-frontend -n staging --timeout=180s || true

          # Restore replicas to original count if we changed it
          if [ "${CURRENT_REPLICAS}" -gt 1 ]; then
            echo "Restoring ecommerce-frontend replicas to ${CURRENT_REPLICAS}"
            kubectl scale deployment ecommerce-frontend -n staging --replicas=${CURRENT_REPLICAS} || true
            kubectl rollout status deployment/ecommerce-frontend -n staging --timeout=180s || true
          fi

      - name: Wait for LoadBalancer hostname (staging service)
        run: |
          for i in {1..20}; do
            HOST=$(kubectl get svc ecommerce-frontend -n staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$HOST" ]; then
              echo "FRONTEND_HOST=$HOST" >> $GITHUB_ENV
              echo "Found LB host: $HOST"; break
            fi
            echo "Waiting for LB host... ($i)"; sleep 15
          done
          if [ -z "$HOST" ]; then echo "LoadBalancer hostname not available"; exit 1; fi

      - name: Probe backend API (API Gateway integration)
        run: |
          mkdir -p backend/tests/e2e-ui/artifacts || true
          echo "Reading API_URL from frontend deployment env..."
          API_URL=$(kubectl get deploy ecommerce-frontend -n staging -o jsonpath='{.spec.template.spec.containers[?(@.name=="frontend")].env[?(@.name=="API_URL")].value}')
          echo "API_URL: $API_URL" | tee backend/tests/e2e-ui/artifacts/backend-probe-url.txt
          if [ -z "$API_URL" ]; then
            echo "API_URL vazio no deployment; não é possível sondar backend" | tee backend/tests/e2e-ui/artifacts/backend-probe-error.txt
            exit 1
          fi

          PROBE_URL="$API_URL/api/products"
          echo "Probing: $PROBE_URL"
          set -o pipefail
          curl -sv --connect-timeout 5 --max-time 15 "$PROBE_URL" \
            -o backend/tests/e2e-ui/artifacts/backend-probe-body.json \
            -D backend/tests/e2e-ui/artifacts/backend-probe-headers.txt \
            2> backend/tests/e2e-ui/artifacts/backend-probe-curl.txt || PROBE_RC=$?

          # Extract HTTP status code
          STATUS=$(awk 'toupper($1) ~ /^HTTP\// {code=$2} END{print code+0}' backend/tests/e2e-ui/artifacts/backend-probe-headers.txt 2>/dev/null || echo 0)
          echo "HTTP STATUS: $STATUS" | tee -a backend/tests/e2e-ui/artifacts/backend-probe-headers.txt
          if [ -n "$PROBE_RC" ]; then
            echo "Curl retornou código $PROBE_RC" | tee backend/tests/e2e-ui/artifacts/backend-probe-error.txt
            exit 1
          fi
          if [ "$STATUS" -lt 200 ] || [ "$STATUS" -ge 400 ]; then
            echo "Backend probe falhou com status $STATUS" | tee backend/tests/e2e-ui/artifacts/backend-probe-error.txt
            exit 1
          fi
          echo "Backend OK (status $STATUS)"

      - name: Run E2E tests against staging
        run: |
          # Create artifacts dir for diagnostic outputs
          mkdir -p backend/tests/e2e-ui/artifacts || true

          # Use the runtime FRONTEND_HOST set by the previous step (written to GITHUB_ENV).
          echo "FRONTEND_HOST runtime value: '$FRONTEND_HOST'"
          if [ -z "$FRONTEND_HOST" ]; then
            echo "FRONTEND_HOST is empty - cannot collect TLS info" > backend/tests/e2e-ui/artifacts/tls-error.txt
          else
            # Collect TLS diagnostic information from the LB before running E2E tests.
            echo "Collecting TLS diagnostic info for: $FRONTEND_HOST"
            # Record timestamp and resolver info
            date > backend/tests/e2e-ui/artifacts/tls-when.txt || true
            echo "Resolver info:" > backend/tests/e2e-ui/artifacts/tls-resolver.txt || true
            # Try to resolve the hostname
            if command -v nslookup >/dev/null 2>&1; then
              nslookup "$FRONTEND_HOST" >> backend/tests/e2e-ui/artifacts/tls-resolver.txt 2>&1 || true
            fi
            getent hosts "$FRONTEND_HOST" >> backend/tests/e2e-ui/artifacts/tls-resolver.txt 2>&1 || true

            # Save curl headers and TLS handshake traces with conservative timeouts so we don't hang
            curl -svI --connect-timeout 10 --max-time 20 "https://$FRONTEND_HOST" -D backend/tests/e2e-ui/artifacts/ssl-headers.txt -o backend/tests/e2e-ui/artifacts/ssl-body.html || true

            # Save openssl s_client cert chain and connection diagnostics using timeout to avoid hangs
            echo "-----BEGIN OPENSSL S_CLIENT OUTPUT-----" > backend/tests/e2e-ui/artifacts/openssl-s_client.txt || true
            echo "Connecting to $FRONTEND_HOST:443" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt || true
            # Use timeout (coreutils) to limit openssl runtime
            if command -v timeout >/dev/null 2>&1; then
              timeout 20 openssl s_client -showcerts -servername "$FRONTEND_HOST" -connect "$FRONTEND_HOST:443" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt 2>&1 || true
            else
              openssl s_client -showcerts -servername "$FRONTEND_HOST" -connect "$FRONTEND_HOST:443" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt 2>&1 || true
            fi
            echo "-----END OPENSSL S_CLIENT OUTPUT-----" >> backend/tests/e2e-ui/artifacts/openssl-s_client.txt || true
          fi

          # Install Chromium on the runner (ubuntu-latest). Using apt-get because
          # snap may not be available in the Actions image. We then point
          # PUPPETEER_EXECUTABLE_PATH at the system binary so puppeteer uses it.
          sudo apt-get update && sudo apt-get install -y chromium-browser chromium
          if [ -x "/usr/bin/chromium-browser" ]; then
            export PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
          elif [ -x "/usr/bin/chromium" ]; then
            export PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium
          else
            echo "No system chromium binary found; allowing puppeteer to download its browser"
            export PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=''
          fi

          # Export FRONTEND_URL at runtime so Puppeteer sees the correct host
          export FRONTEND_URL="http://$FRONTEND_HOST"

          # Run the E2E tests
          cd backend/tests/e2e-ui
          npm ci
          npm run test

      - name: Debug list E2E artifact directory (for troubleshooting)
        if: ${{ always() }}
        run: |
          echo "Workspace: $(pwd)"
          echo "Listing backend/tests/e2e-ui (top level)" || true
          ls -la backend/tests/e2e-ui || true
          echo "Listing backend/tests/e2e-ui/artifacts (if exists)" || true
          ls -la backend/tests/e2e-ui/artifacts || true
          echo "Recursive find (show up to 3 levels)" || true
          find backend/tests/e2e-ui -maxdepth 3 -type f -print || true

      - name: Upload E2E artifacts (if any)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: e2e-artifacts
          path: backend/tests/e2e-ui/artifacts/**

      - name: Collect K8s diagnostics on failure (staging)
        if: ${{ failure() }}
        run: |
          set -e
          NS=staging
          DIAG_DIR=k8s-diag
          mkdir -p "$DIAG_DIR"

          echo "== Get all resources =="
          kubectl get all -n "$NS" -o wide > "$DIAG_DIR/get-all.txt" 2>&1 || true

          echo "== Service and Endpoints =="
          kubectl describe svc ecommerce-frontend -n "$NS" > "$DIAG_DIR/svc-ecommerce-frontend.txt" 2>&1 || true
          kubectl get endpoints ecommerce-frontend -n "$NS" -o wide > "$DIAG_DIR/endpoints-ecommerce-frontend.txt" 2>&1 || true
          # Backend alias (se existir)
          kubectl get svc backend -n "$NS" >/dev/null 2>&1 && kubectl describe svc backend -n "$NS" > "$DIAG_DIR/svc-backend-alias.txt" 2>&1 || true

          echo "== Deployment, ReplicaSets e Rollout =="
          kubectl describe deploy ecommerce-frontend -n "$NS" > "$DIAG_DIR/describe-deploy-ecommerce-frontend.txt" 2>&1 || true
          kubectl rollout history deployment/ecommerce-frontend -n "$NS" > "$DIAG_DIR/rollout-history-ecommerce-frontend.txt" 2>&1 || true
          kubectl get rs -n "$NS" -l app=ecommerce-frontend -o name | while read RS; do
            [ -z "$RS" ] && continue; kubectl describe "$RS" -n "$NS" > "$DIAG_DIR/describe-${RS//\//-}.txt" 2>&1 || true
          done

          echo "== Pods (describe + logs) =="
          kubectl get pods -n "$NS" -l app=ecommerce-frontend -o name | tee "$DIAG_DIR/pod-list.txt"
          kubectl get pods -n "$NS" -l app=ecommerce-frontend -o name | while read P; do
            [ -z "$P" ] && continue;
            SAFE=${P//\//-}
            kubectl describe "$P" -n "$NS" > "$DIAG_DIR/describe-${SAFE}.txt" 2>&1 || true
            kubectl logs "$P" -n "$NS" --all-containers=true --tail=2000 > "$DIAG_DIR/logs-${SAFE}.txt" 2>&1 || true
          done

          echo "== Events (ordenado) =="
          kubectl get events -n "$NS" --sort-by=.lastTimestamp > "$DIAG_DIR/events.txt" 2>&1 || kubectl get events -n "$NS" > "$DIAG_DIR/events.txt" 2>&1 || true

          echo "== Nodes relacionados e CNI =="
          # Descrever nós onde os pods do frontend estão agendados
          for NODE in $(kubectl get pods -n "$NS" -l app=ecommerce-frontend -o jsonpath='{.items[*].spec.nodeName}'); do
            [ -z "$NODE" ] && continue; kubectl describe node "$NODE" > "$DIAG_DIR/describe-node-${NODE}.txt" 2>&1 || true
          done
          # Logs dos pods aws-node (CNI) em kube-system
          kubectl get pods -n kube-system -l k8s-app=aws-node -o name | while read CNI; do
            [ -z "$CNI" ] && continue; SAFE=${CNI//\//-}; kubectl logs "$CNI" -n kube-system --tail=2000 > "$DIAG_DIR/kube-system-${SAFE}-logs.txt" 2>&1 || true
          done

      - name: Upload K8s diagnostics (staging)
        if: ${{ failure() }}
        uses: actions/upload-artifact@v4
        with:
          name: k8s-staging-diagnostics
          path: k8s-diag/**

  promote-to-prod:
    name: Promote to production (gated)
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-and-test-staging]
    environment: production
    env:
      KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure Kubeconfig from secret (if provided)
        if: ${{ env.KUBE_CONFIG != '' }}
        run: |
          echo "${{ env.KUBE_CONFIG }}" | base64 --decode > $GITHUB_WORKSPACE/kubeconfig
          sed -E -i 's/client.authentication\.kxs\.io/client.authentication.k8s.io/g' $GITHUB_WORKSPACE/kubeconfig || true
          sed -E -i 's/client.authentication.k8s.io\/v[0-9a-zA-Z._-]*/client.authentication.k8s.io\/v1/g' $GITHUB_WORKSPACE/kubeconfig || true
          awk '{ print $0;
            if ($0 ~ /^[[:space:]]*exec:[[:space:]]*$/) {
              m = match($0, /[^ \t]/);
              indent = (m>1) ? substr($0,1,m-1) : "";
              if (getline nxt) {
                print indent "  interactiveMode: Never";
                print nxt;
              }
            }
          }' $GITHUB_WORKSPACE/kubeconfig > $GITHUB_WORKSPACE/kubeconfig.fixed && mv $GITHUB_WORKSPACE/kubeconfig.fixed $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure AWS credentials for kubeconfig exec-plugin
        if: ${{ env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Refresh kubeconfig via AWS (ensure exec-plugin uses job credentials)
        if: ${{ env.AWS_ACCESS_KEY_ID != '' && env.EKS_CLUSTER_NAME != '' }}
        run: |
          echo "Refreshing kubeconfig using AWS CLI so exec-plugin picks up credentials"
          aws sts get-caller-identity || true
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }} --kubeconfig $GITHUB_WORKSPACE/kubeconfig
          echo "KUBECONFIG=$GITHUB_WORKSPACE/kubeconfig" >> $GITHUB_ENV

      - name: Configure Kubeconfig via AWS (if KUBE_CONFIG not provided)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig using eksctl (when AWS creds available)
        if: ${{ env.KUBE_CONFIG == '' && env.AWS_ACCESS_KEY_ID != '' }}
        run: |
          if [ -z "${{ env.EKS_CLUSTER_NAME }}" ]; then echo "EKS_CLUSTER_NAME secret is required when KUBE_CONFIG is not set" && exit 1; fi
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region ${{ env.AWS_REGION }}

      - name: Set production image and rollout
        run: |
          echo "Promotion: updating production deployment to use staging image"
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI=${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/ecommerce-ecr-frontend
          IMAGE_SHA=${{ needs.build-and-push.outputs.image_sha }}
          kubectl set image deployment/ecommerce-frontend frontend=$ECR_URI:${IMAGE_SHA} --record
          kubectl rollout status deployment/ecommerce-frontend --timeout=180s
